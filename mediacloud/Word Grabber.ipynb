{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sources and \"Interesting Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mediacloud\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MY_API_KEY = '0b048304d2f7398cb91248b7e07b3b153d32840c1a8c42ab4006f58aaa8a440a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_media = [\"New York Times\", \"NPR\", \"Politico\", \"CNN\", \"American Civil Liberties Union\"]\n",
    "left_fake_media = [\"If You Only News\", \"Occupy Democrats\", \"FWD Now\", \"Jezebel\", \"Blue Tribune\"]\n",
    "right_media = [\"Fox News\", \"Daily Telegraph\", \"Chicago Tribune\", \"Forbes\", \"Washington Times\"]\n",
    "right_fake_media = [\"Russia Today\", \"InfoWars\", \"Natural News\", \"DCLeaks\", \"Breitbart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_interesting_words = ['Trump', 'Obama', 'fake news', 'terrorist', 'Kavanaugh', 'midterm', 'Republican', 'Democrat', 'election',\n",
    "                     'Russia', 'Jeff Sessions', 'Attorney General', 'tolerance', 'racism', 'sexism', 'gender', 'snowflake',\n",
    "                     'shooting', 'massacre', 'guns', 'abortion', 'radical', 'leftwing', 'rightwing', 'queer', 'gay', 'religion',\n",
    "                     'healthcare', 'universal', 'immigrant', 'refugee', 'Syria', 'education']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Media Names to Mediacloud ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv_media(media_name, rows=100):\n",
    "    \"\"\"\n",
    "    Args: media_names: media source name (can have multiple) in list format\n",
    "          chunk_size: max # of media sources that can be written per iteration\n",
    "    Write the media_id, url, and names of the given media_name(s) in a CSV file\n",
    "    \"\"\"\n",
    "    media = []\n",
    "    media_idx = 0\n",
    "    last_media_id = 0\n",
    "\n",
    "    fieldnames = [\n",
    "        u'media_id',\n",
    "        u'url',\n",
    "        u'name'\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        params = { 'last_media_id': last_media_id, 'rows': rows, 'name': media_name[media_idx], 'key': MY_API_KEY }\n",
    "        media_list_call = 'https://api.mediacloud.org/api/v2/media/list'\n",
    "        r = requests.get( media_list_call, params = params, headers = { 'Accept': 'application/json'} )\n",
    "        data = r.json()\n",
    "        print (\"start:{} num_media_sources:{}\".format(last_media_id, len(data)))\n",
    "\n",
    "        if not len(data):\n",
    "\n",
    "            # If there are more media_names, write what we have to csv file and continue\n",
    "            path_name = './csv_storage/media.csv'\n",
    "            with open( path_name, 'a', newline=\"\") as csvfile:\n",
    "                print (\"\\nOpened file: Dumping media source content for {}\\n\".format(media_name[media_idx]))\n",
    "\n",
    "                # Flush media buffer to csv file\n",
    "                cwriter = csv.DictWriter( csvfile, fieldnames, extrasaction='ignore')\n",
    "\n",
    "                if not os.path.getsize(path_name):\n",
    "                    cwriter.writeheader()\n",
    "\n",
    "                cwriter.writerows( media )\n",
    "\n",
    "            # Continue to next user-inputted media_name\n",
    "            media_idx += 1\n",
    "            last_media_id = 0\n",
    "            media = []\n",
    "            if media_idx < len(media_name):\n",
    "                print (\"Grabbing sources of next media name:{}\\n\".format(media_name[media_idx]))\n",
    "                continue\n",
    "\n",
    "            # Done if no more media sources to get\n",
    "            break\n",
    "\n",
    "\n",
    "        #add to media buffer and search for more media sources similar to current media_name\n",
    "        media.extend( data )\n",
    "        last_media_id = media[-1]['media_id']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:0 rows:100\n",
      "start:651204 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for New York Times\n",
      "\n",
      "Grabbing sources of next media name:NPR\n",
      "\n",
      "start:0 rows:100\n",
      "start:91179 rows:100\n",
      "start:139914 rows:100\n",
      "start:190302 rows:100\n",
      "start:240503 rows:100\n",
      "start:299186 rows:100\n",
      "start:369062 rows:100\n",
      "start:443490 rows:100\n",
      "start:508448 rows:100\n",
      "start:558433 rows:100\n",
      "start:616090 rows:100\n",
      "start:705557 rows:100\n",
      "start:763846 rows:100\n",
      "start:821041 rows:100\n",
      "start:885800 rows:100\n",
      "start:944458 rows:100\n",
      "start:1011207 rows:100\n",
      "start:1013589 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for NPR\n",
      "\n",
      "Grabbing sources of next media name:Politico\n",
      "\n",
      "start:0 rows:100\n",
      "start:345823 rows:100\n",
      "start:958555 rows:100\n",
      "start:990826 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Politico\n",
      "\n",
      "Grabbing sources of next media name:CNN\n",
      "\n",
      "start:0 rows:100\n",
      "start:268057 rows:100\n",
      "start:780894 rows:100\n",
      "start:999468 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for CNN\n",
      "\n",
      "Grabbing sources of next media name:American Civil Liberties Union\n",
      "\n",
      "start:0 rows:100\n",
      "start:27427 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for American Civil Liberties Union\n",
      "\n",
      "start:0 rows:100\n",
      "start:206424 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Ifyouonlynews\n",
      "\n",
      "Grabbing sources of next media name:Occupy Democrats\n",
      "\n",
      "start:0 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Occupy Democrats\n",
      "\n",
      "Grabbing sources of next media name:FWD Now\n",
      "\n",
      "start:0 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for FWD Now\n",
      "\n",
      "Grabbing sources of next media name:Jezebel\n",
      "\n",
      "start:0 rows:100\n",
      "start:1008480 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Jezebel\n",
      "\n",
      "Grabbing sources of next media name:Blue Tribune\n",
      "\n",
      "start:0 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Blue Tribune\n",
      "\n",
      "start:0 rows:100\n",
      "start:359086 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Fox News\n",
      "\n",
      "Grabbing sources of next media name:Daily Telegraph\n",
      "\n",
      "start:0 rows:100\n",
      "start:649810 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Daily Telegraph\n",
      "\n",
      "Grabbing sources of next media name:Chicago Tribune\n",
      "\n",
      "start:0 rows:100\n",
      "start:9 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Chicago Tribune\n",
      "\n",
      "Grabbing sources of next media name:Forbes\n",
      "\n",
      "start:0 rows:100\n",
      "start:982161 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Forbes\n",
      "\n",
      "Grabbing sources of next media name:Washington Times\n",
      "\n",
      "start:0 rows:100\n",
      "start:660372 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Washington Times\n",
      "\n",
      "start:0 rows:100\n",
      "start:305385 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Russia Today\n",
      "\n",
      "Grabbing sources of next media name:InfoWars\n",
      "\n",
      "start:0 rows:100\n",
      "start:959059 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for InfoWars\n",
      "\n",
      "Grabbing sources of next media name:Natural News\n",
      "\n",
      "start:0 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Natural News\n",
      "\n",
      "Grabbing sources of next media name:dcleaks\n",
      "\n",
      "start:0 rows:100\n",
      "start:302379 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for dcleaks\n",
      "\n",
      "Grabbing sources of next media name:Breitbart\n",
      "\n",
      "start:0 rows:100\n",
      "start:915942 rows:100\n",
      "\n",
      "Opened file: Dumping media source content for Breitbart\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_csv_media(left_media)\n",
    "write_csv_media(left_fake_media)\n",
    "write_csv_media(right_media)\n",
    "write_csv_media(right_fake_media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Media_ID's with respect to their names in the above lists. Fill in later.\n",
    "left_media_dict = {}\n",
    "left_fake_media_dict = {}\n",
    "right_media_dict = {}\n",
    "right_fake_media_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counting and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions here to loop through the media_sources, grab word-count information, and the assign a positive/negative sentiment score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_story_word_matrix(media_id, rows=1000):\n",
    "    \"\"\"\n",
    "    Args: media_id: ID of media source to grab word matrices for. Stories in the returned dicts\n",
    "    will only be from this media_id.\n",
    "    \"\"\"\n",
    "    params = {'rows': rows, 'q': 'media_id: {}'.format(media_id), 'key': MY_API_KEY}\n",
    "    word_matrix_call = 'https://api.mediacloud.org/api/v2/stories_public/word_matrix'\n",
    "    r = requests.get(word_matrix_call, params = params, headers = { 'Accept': 'application/json'} )\n",
    "    data = r.json()\n",
    "    return data['word_list'], data['word_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-a9fc7c393e52>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-a9fc7c393e52>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print 'Word list for media source: {}\\n\\n'.format(media_dict[media_id])\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_story_word_matrices_for_all_media(media_dict):\n",
    "    word_lists_and_matrices = []\n",
    "    for media_id in media_dict.keys():\n",
    "        data = get_story_word_matrix(media_id)\n",
    "        print ('Word list for media source: {}\\n\\n'.format(media_dict[media_id]))\n",
    "        print (data['word_list'])\n",
    "        print ('Word matrix of word frequency counts by stories_ids for {}\\n\\n'.format(media_dict[media_id]))\n",
    "        print (data['word_matrix'])\n",
    "        word_lists_and_matrices.append(data)\n",
    "    return word_lists_and_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_story_word_matrices_for_all_media' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-811405c851a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft_media_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_story_word_matrices_for_all_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_media_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mleft_fake_media_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_story_word_matrices_for_all_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_fake_media_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mright_media_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_story_word_matrices_for_all_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_media_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mright_fake_media_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_story_word_matrices_for_all_media\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_fake_media_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_story_word_matrices_for_all_media' is not defined"
     ]
    }
   ],
   "source": [
    "left_media_data = get_story_word_matrices_for_all_media(left_media_dict)\n",
    "left_fake_media_data = get_story_word_matrices_for_all_media(left_fake_media_dict)\n",
    "right_media_data = get_story_word_matrices_for_all_media(right_media_dict)\n",
    "right_fake_media_data = get_story_word_matrices_for_all_media(right_fake_media_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_repeat_words_within_media(data, threshold=10):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with repeat words as a key mapped to the number of stories \n",
    "    they've appeared in as a repeat word.\n",
    "    \"\"\"\n",
    "    stories_ids = data['word_matrix'].keys()\n",
    "    repeat_words = collections.Counter()\n",
    "    for stories_id in stories_ids:\n",
    "        word_ids = data['word_matrix'][stories_id].keys()\n",
    "        for word_id in word_ids:\n",
    "            if data['word_matrix'][stories_id][word_id] >= threshold:\n",
    "                repeat_words[data['word_list'][int(word_id)]] += 1\n",
    "    return repeat_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/Nani/anaconda3/lib/python3.4/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a4ed99f23b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_story_word_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18268\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f654a47d2f95>\u001b[0m in \u001b[0;36mget_story_word_matrices\u001b[0;34m(media_id, rows)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mword_matrix_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://api.mediacloud.org/api/v2/stories_public/word_matrix'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_matrix_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'Accept'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'application/json'\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"start:{} num_stories:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedia_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Nani/anaconda3/lib/python3.4/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Nani/anaconda3/lib/python3.4/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Nani/anaconda3/lib/python3.4/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Nani/anaconda3/lib/python3.4/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "def get_repeat_words_within_bias(bias_data):\n",
    "    repeat_words_across_bias = []\n",
    "    for data in bias_data: \n",
    "        repeat_words = get_repeat_words_within_media(data)\n",
    "        repeat_words_across_bias.append(repeat_words)\n",
    "        \n",
    "    repeat_words_\n",
    "    for repeat_words in repeat_words_across_bias[1:]:\n",
    "        repeat_words_set = set(repeat_words.keys())\n",
    "        if \n",
    "        \n",
    "                \n",
    "    \n",
    "    stem = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_count_for_media(media_id, word, num_words=500, sample_size=100000): \n",
    "    \"\"\"\n",
    "    Args: media_id: ID of media_source to grab word count of interesting words for. \n",
    "    \"\"\"\n",
    "    params = {'num_words': num_words, 'sample_size': sample_size, 'include_stats': 1, \n",
    "              'q': 'media_id: {} AND {}'.format(media_id, word)}\n",
    "    word_count_call = 'https://api.mediacloud.org/api/v2/wc/list'\n",
    "    r = requests.get(word_count_call, params = params, headers = { 'Accept': 'application/json'} )\n",
    "    data = r.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_interesting_word_counts_for_all_media(media_dict):\n",
    "    for media_id in media_dict.keys():\n",
    "        for word in interesting_words:\n",
    "            data = get_word_count_for_media(media_id, word)\n",
    "            print \"Word frequency counts in sentences containing the word {} in {}:\\n\\n\".format(word, media_dict[media_id])\n",
    "            print data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_all_interesting_word_counts_for_all_media(media_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
